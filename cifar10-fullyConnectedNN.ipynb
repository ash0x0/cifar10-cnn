{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit,xlogy\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.ndimage.interpolation import *\n",
    "from numpy import fliplr\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_CIFAR_batch(filename):\n",
    "  \"\"\" load single batch of cifar \"\"\"\n",
    "  with open(filename, 'rb') as f:\n",
    "    datadict = pickle.load(f, encoding='latin1')\n",
    "    X = datadict['data']\n",
    "    Y = datadict['labels']\n",
    "    X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float64\")\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "def load_CIFAR10(ROOT,batch_list=[1,2,3,4,5],batch_size=1000,gray_scale=False):\n",
    "  \"\"\" load all of cifar \"\"\"\n",
    "  total_train_size = len(batch_list)*batch_size\n",
    "  Xtr = np.empty([total_train_size, 32, 32, 3],dtype=np.float64)\n",
    "  Ytr = np.empty([total_train_size,1],dtype=np.int32)\n",
    "\n",
    "  start,end = 0,batch_size\n",
    "  for b in batch_list:\n",
    "    f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n",
    "    \n",
    "    xtmp, ytmp = load_CIFAR_batch(f)\n",
    "    Xtr[start:end,:],Ytr[start:end] = xtmp[:batch_size],ytmp[:batch_size].reshape(batch_size,1)\n",
    "\n",
    "    start += batch_size\n",
    "    end += batch_size\n",
    "\n",
    "  if gray_scale is True:\n",
    "    Xtr = np.mean(Xtr,axis=3)\n",
    "\n",
    "  Xtr = Xtr.reshape(total_train_size,-1)\n",
    "\n",
    "  return Xtr, Ytr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_set(ROOT,gray_scale=False):\n",
    "  x,y = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n",
    "  if gray_scale is True:\n",
    "    x = np.mean(x,axis=3)\n",
    "  x = x.reshape(10000,-1)\n",
    "  return x,y.reshape(10000,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape(xtrain):\n",
    "    dist_type = randint(0,4)\n",
    "    xtrain = xtrain.reshape(-1,32,32,3)\n",
    "    \n",
    "    if dist_type is 0:\n",
    "        xtrain = fliplr(xtrain)\n",
    "    elif dist_type is 1:\n",
    "        angle = randint(-14,14)\n",
    "        xtrain = rotate(xtrain,angle,reshape=False)\n",
    "    elif dist_type is 2:\n",
    "        shift_amt = randint(-14,14)\n",
    "        xtrain = shift(xtrain,(0,0,shift_amt,0),order=0,prefilter=False,mode='nearest')\n",
    "\n",
    "    return xtrain.reshape(-1,3072)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer:\n",
    "\n",
    "    def norm_train(self,x):\n",
    "        self.mean = np.mean(x, axis =0)\n",
    "        self.std = np.std(x, axis = 0)\n",
    "\n",
    "        x -= self.mean\n",
    "        x /= self.std\n",
    "        return x\n",
    "\n",
    "    def norm_test(self,x):\n",
    "        return (x - self.mean)/self.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedNN:\n",
    "\n",
    "    def __init__(self,cost='square_error'):\n",
    "        self.ws = []\n",
    "        self.activ = []\n",
    "        self.cost = cost\n",
    "        self.regularization = 0\n",
    "        self.dropout = []\n",
    "\n",
    "    def set_regularization(self,regularization):\n",
    "        self.regularization = regularization\n",
    "\n",
    "    def add_layer(self,shape,act_func='sigmoid',dropout=0):\n",
    "        self.ws.append(shape)\n",
    "        self.dropout.append(dropout)\n",
    "        if act_func is not None:\n",
    "            self.activ.append(act_func)\n",
    "\n",
    "    def make_weights(self,shape):\n",
    "        return np.random.randn(shape[0]+1,shape[1])/np.sqrt(shape[0]+1)\n",
    "\n",
    "    def train(self,xtr,ytr,vx=None,vy=None,moment=0.2,\n",
    "        lr=0.9,max_epoch=1000,callback=None):\n",
    "        train_error,validation_error = -1,-1\n",
    "        epoch = 0\n",
    "        while self.halt(train_error) and max_epoch > epoch:\n",
    "            train_error = self.__train__(xtr,ytr,moment=moment,lr=lr,epoch=1)\n",
    "            if vx is not None and vy is not None:\n",
    "                validation_error = self.__validation_error__(vx,vy)\n",
    "            epoch += 1\n",
    "            callback(epoch,train_error,validation_error)\n",
    "\n",
    "    def __train__(self,xtr,ytr,moment=0.2,lr=0.9,epoch=100):\n",
    "        output_dim = self.ws[-1].shape[1]\n",
    "        ytr = np.concatenate([ytr == i for i in range(output_dim)],axis=1)\n",
    "\n",
    "        for i in range(epoch):\n",
    "            for xs,ys in self.get_batch(xtr,ytr):\n",
    "                xs = reshape(xs)\n",
    "                error,gradients = self.back_propogation(xs,ys)\n",
    "                self.update_weights(gradients,moment,lr)\n",
    "        return error\n",
    "\n",
    "    def __validation_error__(self,vx,vy):\n",
    "        predict = self.predict(vx)\n",
    "        predict = np.argmax(predict,axis=1).reshape(-1,1)\n",
    "\n",
    "        return np.mean(predict == vy)\n",
    "\n",
    "    def store_weights(self,epoch):\n",
    "        print('saving weights')\n",
    "        for i in range(len(self.ws)):\n",
    "            file = open(\"weights/weight_{}\".format(i), 'wb+')\n",
    "            np.save(file,self.ws[i])\n",
    "            file.close\n",
    "        print('done saving')\n",
    "\n",
    "    def halt(self,loss):\n",
    "        prev_loss = inf,count = 0\n",
    "        while True:\n",
    "            if prev_loss - loss > 1e-3:\n",
    "                count = 0\n",
    "                prev_loss = loss\n",
    "                yield False\n",
    "            elif count > 2:\n",
    "                yield True\n",
    "            else:\n",
    "                count += 1\n",
    "                yield False\n",
    "\n",
    "    def get_batch(self,xtrain,ytrain):\n",
    "        slice_index,slice_size = 0,50\n",
    "\n",
    "        while slice_index < xtrain.shape[0]:\n",
    "            yield xtrain[slice_index:slice_index+slice_size], \\\n",
    "                ytrain[slice_index:slice_index+slice_size]\n",
    "            slice_index += slice_size\n",
    "\n",
    "    def update_weights(self,gradients,moment,lr):\n",
    "        for i in range(len(gradients)):\n",
    "            self.moment[i] *= moment\n",
    "            self.moment[i] += lr*gradients[i]\n",
    "            self.ws[i] -= self.moment[i]\n",
    "\n",
    "    def back_propogation(self,xtr,ytr):\n",
    "        fwd_a,fwd_z = self.__predict__(xtr)\n",
    "        out_error, out_grad = self.out_error(fwd_a[-1],ytr,self.cost)\n",
    "        out_grad = np.multiply(self.activation_gradient(fwd_z[-1],self.activ[-1]),out_grad)\n",
    "        ws_grads = []\n",
    "\n",
    "        for w in range(len(self.ws)-1,-1,-1):\n",
    "            ws_grad = np.dot(out_grad.T, fwd_a[w])\n",
    "            ws_grads.append(ws_grad.T)\n",
    "            if w > 0:\n",
    "                out_grad = np.dot(out_grad,self.ws[w][1:,:].T)\n",
    "                out_grad = np.multiply(out_grad,\n",
    "                    self.activation_gradient(fwd_z[w-1],self.activ[w]))\n",
    "\n",
    "        ws_grads.reverse() \n",
    "\n",
    "        for i in range(len(ws_grads)):\n",
    "            ws_grads[i] /= xtr.shape[0]\n",
    "            ws_grads[i][:,1:] += self.regularization/xtr.shape[0]*self.ws[i][:,1:]\n",
    "\n",
    "        return out_error,ws_grads\n",
    "\n",
    "    def predict(self,x):\n",
    "        hx = self.__predict__(x)[0][-1]\n",
    "        return hx\n",
    "\n",
    "    def __predict__(self,x):\n",
    "        m = x.shape[0]\n",
    "        x = np.c_[np.ones((m,1)), x]\n",
    "        fwd_a,fwd_z = [x],[]\n",
    "        \n",
    "        for layer in range(len(self.ws)):\n",
    "            z = np.dot(fwd_a[-1],self.ws[layer])\n",
    "            a = np.c_[np.ones((m,1)),self.activate(z,self.activ[layer])]\n",
    "            fwd_z.append(z)\n",
    "            fwd_a.append(a)\n",
    "\n",
    "        fwd_a[-1] = fwd_a[-1][:,1:]\n",
    "        return fwd_a,fwd_z\n",
    "\n",
    "    def make_dropout(self):\n",
    "        self.ws_bkup = self.ws\n",
    "        self.ws = []\n",
    "        for i in range(len(self.ws_bkup)):\n",
    "            self.ws.append(self.dropout_weight(self.ws_bkup[i],self.dropout[i]))\n",
    "\n",
    "    def load_weights(self):\n",
    "        for i in range(len(self.ws_bkup)):\n",
    "            nonzero_entry = np.nonzero(self.ws[i])\n",
    "            self.ws_bkup[i][nonzero_entry] = self.ws[i][nonzero_entry]\n",
    "        self.ws = self.ws_bkup\n",
    "\n",
    "    def out_error(self,hx,y,act):\n",
    "        funcs = {\"square_error\":self.square_error,\n",
    "                \"cross_entropy_softmax\":self.cross_entropy_softmax,\n",
    "                \"log_cross_entropy\":self.log_cross_entropy}\n",
    "        return funcs[act](hx,y)\n",
    "\n",
    "    def square_error(self,hx,label):\n",
    "        grad = hx - label\n",
    "        square_diff = np.square(grad).sum()/2\n",
    "        for i in range(len(self.ws)):\n",
    "            square_diff += np.sum(self.regularization/(2*hx.shape[0])*np.square(self.ws[i][:,1:]))\n",
    "        return square_diff, grad\n",
    "\n",
    "    def sigmoid(self,x):\n",
    "        grad = expit(x)\n",
    "        return np.multiply(grad, 1 - grad)\n",
    "\n",
    "    def activate(self,x,act):\n",
    "        funcs = {\"sigmoid\":expit,\n",
    "        \"relu\":self.relu,\n",
    "        \"leaky_relu\":self.leaky_relu,\n",
    "        \"softmax\":self.softmax,\n",
    "        \"elu\":self.elu\n",
    "        }\n",
    "        return funcs[act](x)\n",
    "\n",
    "    def activation_gradient(self,x,act):\n",
    "        funcs = {\"sigmoid\":self.sigmoid,\n",
    "                \"relu\":self.relu_grad,\n",
    "                \"leaky_relu\":self.leaky_relu,\n",
    "                \"softmax\":self.softmax_grad,\n",
    "                \"elu\":self.elu_grad}\n",
    "        return funcs[act](x)\n",
    "\n",
    "    def leaky_relu(self,x):\n",
    "        return np.maximum(x,0.01*x)\n",
    "\n",
    "    def log_cross_entropy(self,hx,y):\n",
    "        error = (-xlogy(y,hx) - xlogy(1-y,1-hx)).sum()/hx.shape[0]\n",
    "        for i in range(len(self.ws)):\n",
    "            error += self.regularization/(2*hx.shape[0]) \\\n",
    "            * np.sum(np.square(self.ws[i][:,1:]))\n",
    "\n",
    "        y = y.astype('float')\n",
    "\n",
    "        grad = np.divide(-y, hx, out=np.zeros_like(y), where= hx!=0)\n",
    "        grad += np.divide(1-y,1-hx, out=np.zeros_like(y), where= (1-hx)!=0)\n",
    "        return error,grad\n",
    "\n",
    "    def cross_entropy_softmax(self,hx,y):\n",
    "        error = -xlogy(y,hx).sum()/hx.shape[0]\n",
    "        for i in range(len(self.ws)):\n",
    "            error += self.regularization/(2*hx.shape[0]) \\\n",
    "            * np.sum(np.square(self.ws[i][:,1:]))\n",
    "\n",
    "        grad = hx - y\n",
    "        return error,grad\n",
    "\n",
    "\n",
    "    def softmax(self,hx):\n",
    "        nterm = np.max(hx,axis=1).reshape(-1,1)\n",
    "        grad = np.exp(hx - nterm)\n",
    "        return grad/grad.sum(axis=1,keepdims=True)\n",
    "\n",
    "    def softmax_grad(self,hx):\n",
    "        return np.ones(hx.shape)\n",
    "\n",
    "    def relu(self,x):\n",
    "        return np.maximum(x,0)\n",
    "\n",
    "    def relu_grad(self,x):\n",
    "        return (x > 0)\n",
    "\n",
    "    def elu(self,x):\n",
    "        alpha = 0.5\n",
    "        return alpha * np.exp(x - np.max(x,axis=1).reshape(-1,1)) - alpha\n",
    "\n",
    "    def elu_grad(self,x):\n",
    "        alpha = 0.5\n",
    "        return (x > 0) + (x < 0) * alpha * np.exp(x - np.max(x,axis=1).reshape(-1,1))\n",
    "\n",
    "    def compile(self):\n",
    "        self.ws,shape = [],self.ws\n",
    "        self.moment = []\n",
    "        for i in range(len(shape)-1):\n",
    "            w = self.make_weights((shape[i],shape[i+1]))\n",
    "            self.moment.append(np.zeros((shape[i]+1,shape[i+1])))\n",
    "            self.ws.append(w)\n",
    "\n",
    "    def dropout_weight(self,weight,drop_prob):\n",
    "        new_weight = np.copy(weight)\n",
    "        drop_prob = int(new_weight.shape[0]*drop_prob)\n",
    "        zeros = np.random.choice(new_weight.shape[0],drop_prob)\n",
    "        new_weight[zeros,:] = 0\n",
    "        return new_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotLearning(epoch,terror,verror):\n",
    "    print(terror,verror)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.seterr(over='raise') \n",
    "batch_list = [1,2,3,4,5]\n",
    "batch_size = 10000\n",
    "gray_scale=False\n",
    "cifar10_dir = 'cifar-10-batches-py'\n",
    "X_train, y_train = load_CIFAR10(cifar10_dir,\n",
    "                    batch_list,batch_size,gray_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Normalizer = Normalizer()\n",
    "X_train = Normalizer.norm_train(X_train)\n",
    "X_test, y_test = load_test_set(cifar10_dir,gray_scale)\n",
    "X_test = Normalizer.norm_test(X_test)\n",
    "m = int(X_train.shape[0]*0.8)\n",
    "X_valid,Y_valid = X_train[m:],y_train[m:]\n",
    "X_train,y_train = X_train[:m],y_train[:m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = FullyConnectedNN(cost=\"cross_entropy_softmax\")\n",
    "classifier.add_layer(3072,None)\n",
    "classifier.add_layer(1400,'relu')\n",
    "classifier.add_layer(500,'relu')\n",
    "classifier.add_layer(10,'softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1845612844 0.3704\n",
      "1.57483722096 0.3983\n",
      "1.5205885045 0.4162\n",
      "1.47711391363 0.4256\n",
      "1.37509798223 0.4362\n",
      "1.57394531492 0.442\n",
      "1.31080961026 0.4537\n",
      "1.25226199612 0.4546\n",
      "1.20863445556 0.4624\n",
      "1.53456747585 0.4724\n",
      "1.36858301731 0.4668\n",
      "1.60713252217 0.4792\n",
      "1.14111242273 0.4816\n",
      "1.1131972957 0.4846\n",
      "1.42879895464 0.488\n",
      "1.07325462647 0.4917\n",
      "1.75055721842 0.494\n",
      "2.25237570162 0.497\n",
      "1.03703369846 0.4953\n",
      "1.04396258935 0.4999\n",
      "2.0036986498 0.4991\n",
      "1.00250091988 0.51\n",
      "1.42246715687 0.508\n",
      "1.43560480594 0.5086\n",
      "0.991966226145 0.5078\n",
      "0.970985299151 0.5119\n",
      "1.45963054082 0.5139\n",
      "1.20394618954 0.5109\n",
      "1.58103358549 0.5151\n",
      "1.59804553608 0.5191\n",
      "1.33175688103 0.5176\n",
      "0.973960576991 0.5155\n",
      "1.24021109318 0.5204\n",
      "1.38693384686 0.53\n",
      "1.24239412582 0.5208\n",
      "1.25692925325 0.5242\n",
      "2.29611160613 0.5182\n",
      "1.1970999445 0.5177\n",
      "1.67533411415 0.523\n",
      "1.74915821525 0.5278\n",
      "1.04684196046 0.5309\n",
      "0.985060016568 0.5215\n",
      "0.904312024892 0.5281\n",
      "0.896095379286 0.5309\n",
      "0.865612443526 0.5316\n",
      "0.85338488251 0.5354\n",
      "1.19865074188 0.5334\n",
      "0.86854409893 0.5343\n",
      "0.803226707477 0.5333\n",
      "0.80888517891 0.5413\n"
     ]
    }
   ],
   "source": [
    "classifier.train(X_train,y_train,vx=X_valid,\n",
    "    vy=Y_valid,moment=0.6,lr=1e-3,max_epoch=50,\n",
    "    callback=plotLearning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy  0.700725\n",
      "validtion accuracy  0.5413\n",
      "testing accuracy  0.5347\n"
     ]
    }
   ],
   "source": [
    "hx = np.argmax(classifier.predict(X_train),axis=1).reshape(-1,1)\n",
    "print(\"training accuracy \",np.mean(hx == y_train))\n",
    "\n",
    "hx = np.argmax(classifier.predict(X_valid),axis=1).reshape(-1,1)\n",
    "print(\"validation accuracy \",np.mean(hx == Y_valid))\n",
    "\n",
    "hx = np.argmax(classifier.predict(X_test),axis=1).reshape(-1,1)\n",
    "print(\"testing accuracy \",np.mean(hx == y_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
